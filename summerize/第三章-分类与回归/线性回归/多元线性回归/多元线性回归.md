# 多元线性回归（Multi-variate Linear Regression）

## 前置知识

### 1. 向量与矩阵
- 向量：一组有序数值，可表示为 $\mathbf{x} = (x_1, x_2, ..., x_d)^T$
- 矩阵：由多个向量组成的二维数组，常用于批量数据表示。
- 矩阵转置：将行列互换，记为 $\mathbf{X}^T$
- 矩阵乘法：$A_{m \times n} B_{n \times p}$ 结果为 $C_{m \times p}$
- 矩阵逆：$A^{-1}$，满足 $A A^{-1} = I$

### 2. 微积分中的梯度
- 对向量 $\mathbf{w}$ 求导，结果为梯度向量，表示函数在各方向的变化率。

### 3. 满秩（Full Rank）和最大线性无关行（或列）

- 一个矩阵的“秩”就是它里面最多能挑出多少行（或列）彼此没有“重复信息”。这些没有“重复信息”的行（或列）叫做“线性无关”。
- 如果一个 $n \times n$ 的方阵，能挑出 $n$ 行彼此线性无关，这个矩阵就叫“满秩”。
- 在多元线性回归中，$\mathbf{X}^T\mathbf{X}$ 满秩，说明所有特征列都能提供独立的信息，参数解唯一且可逆。
- 如果不满秩，说明有些特征其实是“重复”或“可以由其他特征算出来的”，这时解就不唯一。

#### 举例说明：

假设有三个学生的成绩数据，每个学生有两个特征：语文分数和数学分数。

| 学生 | 语文 | 数学 |
|------|------|------|
| 1    | 80   | 90   |
| 2    | 85   | 95   |
| 3    | 90   | 100  |

这时，语文和数学分数是线性无关的，因为不能用语文分数加减乘除得到数学分数。

但如果有第三个特征“总分”，且总分 = 语文 + 数学，那么“总分”这一列就是前两列的线性组合，这三列就不是线性无关的。

所以：
- 线性无关：每一列都带来新信息，不能用其他列算出来。
- 最大线性无关行（或列）：能挑出最多多少列彼此线性无关，这个数量就是“秩”。
- 满秩：如果能挑出和列数一样多的线性无关列，就是满秩。

#### 如何判断矩阵是否满秩？
- 计算方法：
  1. 把矩阵做高斯消元（行变换），看能得到多少个不全为0的行。
  2. 或者用行列式：对于 $n \times n$ 方阵，行列式不为0，则满秩。
  3. 用数学软件（如Python的numpy库）可以直接用 `numpy.linalg.matrix_rank(X)` 计算秩，若等于列数则满秩。
- 公式：
  - 若 $\text{rank}(A) = n$，$A$ 为 $n \times n$ 方阵，则 $A$ 满秩。

---

## 问题描述

给定 $m$ 个样本，每个样本有 $d$ 个特征：
- $\mathbf{x}_i = (x_{i1}, x_{i2}, ..., x_{id})$
- $y_i \in \mathbb{R}$

目标：找到参数 $\mathbf{w}$ 和 $b$，使得
$$
f(\mathbf{x}_i) = \mathbf{w}^T \mathbf{x}_i + b \approx y_i
$$

将 $\mathbf{w}$ 和 $b$ 合并为 $\hat{\mathbf{w}} = (\mathbf{w}; b)$，将每个样本扩展为 $(\mathbf{x}_i^T, 1)$，数据集矩阵化：

$$
\mathbf{X} = \begin{pmatrix}
  x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
  x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
  \vdots & \vdots & \ddots & \vdots & \vdots \\
  x_{m1} & x_{m2} & \cdots & x_{md} & 1
\end{pmatrix}
$$

$$
\mathbf{y} = (y_1, y_2, ..., y_m)^T
$$

---

## 损失函数（最小二乘法）

采用均方误差损失：
$$
E_{\hat{\mathbf{w}}} = (\mathbf{y} - \mathbf{X}\hat{\mathbf{w}})^T (\mathbf{y} - \mathbf{X}\hat{\mathbf{w}})
$$

目标：
$$
\hat{\mathbf{w}}^* = \arg\min_{\hat{\mathbf{w}}} E_{\hat{\mathbf{w}}}
$$

---

## 推导过程

1. 展开损失函数：
   $$
   E_{\hat{\mathbf{w}}} = (\mathbf{y} - \mathbf{X}\hat{\mathbf{w}})^T (\mathbf{y} - \mathbf{X}\hat{\mathbf{w}})
   $$
   展开：
   $$
   = \mathbf{y}^T\mathbf{y} - 2\mathbf{y}^T\mathbf{X}\hat{\mathbf{w}} + \hat{\mathbf{w}}^T\mathbf{X}^T\mathbf{X}\hat{\mathbf{w}}
   $$
2. 对 $\hat{\mathbf{w}}$ 求梯度：
   $$
   \frac{\partial E_{\hat{\mathbf{w}}}}{\partial \hat{\mathbf{w}}} = 2\mathbf{X}^T\mathbf{X}\hat{\mathbf{w}} - 2\mathbf{X}^T\mathbf{y}
   $$
3. 令梯度为零，得到正规方程：
   $$
   \mathbf{X}^T\mathbf{X}\hat{\mathbf{w}} = \mathbf{X}^T\mathbf{y}
   $$
4. 若 $\mathbf{X}^T\mathbf{X}$ 满秩且可逆：
   $$
   \hat{\mathbf{w}}^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
   $$
5. 若 $\mathbf{X}^T\mathbf{X}$ 不满秩（列线性相关），则存在无穷多组参数都能拟合数据，此时需引入正则化等方法。

---

## 结论与说明
- 多元线性回归将所有特征和偏置合并为向量，利用矩阵运算统一表达。
- 通过最小化均方误差，推导出参数的闭式解。
- 关键步骤涉及矩阵求导、正规方程和矩阵求逆。
- 若数据存在共线性或特征冗余，正规方程无唯一解，可用正则化方法解决。

---

本推导适用于多元线性回归的参数估计与理解。