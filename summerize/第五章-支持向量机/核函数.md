在低维空间，存在描述点之间距离的矩阵，d是x1，2的距离
   x1 x2 
x1 0  d
x2 d  0
这个矩阵是对称半正定的

使用ø(x)将低维的点映射到高维，高维肯定也存在描述点之间距离的矩阵，类似上面。所以如果存在对称半正定的距离矩阵，我们就可以说这个矩阵对应一个高维的空间；如果一个核函数k(x1,x2)对应的矩阵是一个对称半正定的矩阵，那么这个核函数肯定对应一个距离空间，也就是高维的空间。

“Mercer 定理”（Mercer's Theorem）。Mercer 定理说明：只要核函数对应的核矩阵在任意有限样本下都是对称半正定的，就存在一个高维空间的特征映射，使得该核函数可以看作是高维空间中的内积。因此，核方法可以在不显式计算高维映射的情况下，利用核函数进行运算。

也就是k(xi,xj) = ø(x)^T ø(x)
这样就不需要算高维内积了，计算开销陡降

但是上述内容说明存在核函数，不代表所有核函数都能保证等式成立，所以要从多个核函数中寻找最接近的

---

## 核化判别分析公式推导

1. 线性判别分析（LDA）在原始空间的目标是最大化类间散度与类内散度的比值：
   
   $$
   \max_w J(w) = \frac{w^T S_b w}{w^T S_w w}
   $$
   其中 $S_b$ 和 $S_w$ 分别是类间和类内散度矩阵。

2. 核化（Kernelization）思想是将样本通过非线性映射 $\phi(x)$ 投影到高维特征空间，在高维空间做线性判别分析：
   
   $$
   \max_w J(w) = \frac{w^T S_b^\phi w}{w^T S_w^\phi w}
   $$

3. 由于高维空间的 $w$ 可以表示为所有训练样本的线性组合，即 $w = \sum_{i=1}^m \alpha_i \phi(x_i)$，于是判别函数可写为：
   
   $$
   h(x) = w^T \phi(x) = \sum_{i=1}^m \alpha_i \kappa(x, x_i)
   $$
   其中 $\kappa(x, x_i) = \phi(x)^T \phi(x_i)$ 是核函数。

4. 将 $w$ 的表达式代入目标函数，原问题转化为关于 $\alpha$ 的优化问题：
   
   $$
   \max_\alpha J(\alpha) = \frac{\alpha^T M \alpha}{\alpha^T N \alpha}
   $$
   其中 $M$ 和 $N$ 是由核函数计算得到的矩阵。

5. 这样，原本需要在高维空间计算的内积和优化问题，通过核函数和核矩阵在低维空间完成，极大简化了计算。

---

## KLDA（核化LDA）公式详细推导与$w$的解释

### 1. 线性判别分析（LDA）目标
LDA的目标是在投影方向$w$上最大化类间散度与类内散度的比值：

$$
\max_w J(w) = \frac{w^T S_b w}{w^T S_w w}
$$
其中$S_b$是类间散度矩阵，$S_w$是类内散度矩阵。

### 2. 样本映射到高维空间
核方法的思想是将原始样本$x$通过非线性映射$\phi(x)$映射到高维特征空间$\mathcal{F}$，在高维空间做线性判别分析：

$$
\max_w J(w) = \frac{w^T S_b^\phi w}{w^T S_w^\phi w}
$$

### 3. $w$的线性组合表示
在高维空间$\mathcal{F}$中，$w$是一个向量。根据表示定理（Representer Theorem），在以核函数为内积的Hilbert空间中，最优解$w$一定可以表示为所有训练样本映射的线性组合：

$$
\boxed{w = \sum_{i=1}^m \alpha_i \phi(x_i)}
$$

**原因**：
- $w$的最优解一定在训练样本张成的子空间内。
- 这是因为目标函数和约束都只与$\phi(x_i)$的内积相关。
- 这也是核方法能用核函数替代高维内积的理论基础。

### 4. 判别函数的核化表达
将$w$的表达式代入判别函数：

$$
\begin{align*}
h(x) &= w^T \phi(x) \\
     &= \left(\sum_{i=1}^m \alpha_i \phi(x_i)\right)^T \phi(x) \\
     &= \sum_{i=1}^m \alpha_i \underbrace{\phi(x_i)^T \phi(x)}_{\kappa(x_i, x)} \\
     &= \sum_{i=1}^m \alpha_i \kappa(x, x_i)
\end{align*}
$$

### 5. 目标函数的核化表达
同理，将$w$用$\alpha$表示后，目标函数也可以全部用核函数表达：

$$
\max_\alpha J(\alpha) = \frac{\alpha^T M \alpha}{\alpha^T N \alpha}
$$
其中$M$和$N$都是由核函数$\kappa(x_i, x_j)$计算得到的矩阵。

### 6. 总结
- $w$在高维空间中可以表示为所有训练样本映射的线性组合，这是核方法的理论基础。
- 通过核函数，所有高维空间的内积都可以用低维空间的核函数计算，极大简化了计算。
- KLDA的优化问题最终转化为关于$\alpha$的广义Rayleigh商问题。