# BP算法推导详细过程

## 1. 符号说明
- $x_i$：输入层第 $i$ 个神经元的输出
- $v_{ih}$：输入层到隐层的权重，$i$ 到 $h$
- $\alpha_h$：隐层第 $h$ 个神经元的输入，$\alpha_h = \sum_{i=1}^d v_{ih} x_i$
- $b_h$：隐层第 $h$ 个神经元的输出，$b_h = f(\alpha_h - \gamma_h)$
- $\gamma_h$：隐层第 $h$ 个神经元的阈值
- $w_{hj}$：隐层到输出层的权重，$h$ 到 $j$
- $\beta_j$：输出层第 $j$ 个神经元的输入，$\beta_j = \sum_{h=1}^q w_{hj} b_h$
- $\theta_j$：输出层第 $j$ 个神经元的阈值
- $\hat{y}_j$：输出层第 $j$ 个神经元的输出，$\hat{y}_j = f(\beta_j - \theta_j)$
- $E_k$：第 $k$ 个样本的误差
- $\eta$：学习率
- $g_j$：输出层第 $j$ 个神经元的梯度项
- $e_h$：隐层第 $h$ 个神经元的梯度项

## 2. 目标
以 $w_{hj}$ 为例，推导如何通过梯度下降法更新权重。

## 3. 误差函数
设第 $k$ 个样本的误差为：
$$
E_k = \frac{1}{2} \sum_j (y_j^k - \hat{y}_j^k)^2
$$

## 4. 梯度下降更新公式

梯度下降的思想是：每次沿着误差函数 $E_k$ 关于参数 $w_{hj}$ 的负梯度方向调整参数，使误差减小。

具体推导如下：

1. 目标是最小化误差 $E_k$，即 $\min E_k$。
2. 误差 $E_k$ 关于 $w_{hj}$ 的梯度为 $\frac{\partial E_k}{\partial w_{hj}}$，表示 $w_{hj}$ 增大时 $E_k$ 的变化率。
3. 为了让 $E_k$ 下降，$w_{hj}$ 应该沿着负梯度方向调整：
   $$
   w_{hj}^{(new)} = w_{hj}^{(old)} - \eta \frac{\partial E_k}{\partial w_{hj}}
   $$
   其中 $\eta$ 是学习率，控制每次更新的步长。
4. 记增量 $\Delta w_{hj} = w_{hj}^{(new)} - w_{hj}^{(old)}$，则有：
   $$
   \Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}}
   $$

## 5. 链式法则分解
$w_{hj}$ 影响 $\beta_j$，进而影响 $\hat{y}_j$，最终影响 $E_k$，所以：
$$
\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot \frac{\partial \hat{y}_j^k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial w_{hj}}
$$

## 6. 逐项计算

### 6.1 $\frac{\partial E_k}{\partial \hat{y}_j^k}$
$$
\frac{\partial E_k}{\partial \hat{y}_j^k} = -(y_j^k - \hat{y}_j^k)
$$

### 6.2 $\frac{\partial \hat{y}_j^k}{\partial \beta_j}$
假设激活函数为 sigmoid：
$$
\hat{y}_j^k = f(\beta_j - \theta_j) = \frac{1}{1 + e^{-(\beta_j - \theta_j)}}
$$
其导数：
$$
\frac{d f(x)}{dx} = f(x)(1 - f(x))
$$
所以：
$$
\frac{\partial \hat{y}_j^k}{\partial \beta_j} = \hat{y}_j^k (1 - \hat{y}_j^k)
$$

### 6.3 $\frac{\partial \beta_j}{\partial w_{hj}}$
$$
\beta_j = \sum_{h=1}^q w_{hj} b_h \implies \frac{\partial \beta_j}{\partial w_{hj}} = b_h
$$

## 7. 梯度项 $g_j$
令：
$$
g_j = \frac{\partial E_k}{\partial \hat{y}_j^k} \cdot \frac{\partial \hat{y}_j^k}{\partial \beta_j} = (\hat{y}_j^k - y_j^k) \cdot \hat{y}_j^k (1 - \hat{y}_j^k)
$$

## 8. 权重更新公式
$$
\Delta w_{hj} = -\eta \frac{\partial E_k}{\partial w_{hj}} = -\eta g_j b_h
$$

## 9. 隐层梯度项 $e_h$ 推导
隐层的误差项：
$$
e_h = \frac{\partial E_k}{\partial b_h} \cdot \frac{\partial b_h}{\partial \alpha_h}
$$

其中：
$$
\frac{\partial E_k}{\partial b_h} = \sum_{j=1}^l \frac{\partial E_k}{\partial \beta_j} \cdot \frac{\partial \beta_j}{\partial b_h} = \sum_{j=1}^l g_j w_{hj}
$$
$$
\frac{\partial b_h}{\partial \alpha_h} = b_h (1 - b_h)
$$
所以：
$$
e_h = b_h (1 - b_h) \sum_{j=1}^l w_{hj} g_j
$$

## 10. 其它参数的更新
- 输出层阈值：$\Delta \theta_j = -\eta g_j$
- 隐层到输入层权重：$\Delta v_{ih} = \eta e_h x_i$
- 隐层阈值：$\Delta \gamma_h = -\eta e_h$

---

以上为BP算法的详细推导过程和每个符号的含义。
