# 信息熵（Entropy）公式与最小值推导

假设当前样本集合 $D$ 中第 $k$ 类样本所占比例为 $p_k$，则 $D$ 的信息熵定义为：

$$
Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_2 p_k
$$

其中 $|\mathcal{Y}|$ 表示类别的总数。

- 约定：若 $p_k = 0$，则 $p_k \log_2 p_k = 0$。

## 信息熵最小值的严格推导

设 $p_k$ 满足 $\sum_{k=1}^{|\mathcal{Y}|} p_k = 1$ 且 $p_k \geq 0$。

我们用拉格朗日乘子法求 $Ent(D)$ 的最小值：

构造拉格朗日函数：
$$
L(p_1,\ldots,p_{|\mathcal{Y}|},\lambda) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_2 p_k + \lambda \left(\sum_{k=1}^{|\mathcal{Y}|} p_k - 1\right)
$$

对每个 $p_j$ 求偏导并令其为零：
$$
\frac{\partial L}{\partial p_j} = -\log_2 p_j - \frac{1}{\ln 2} + \lambda = 0
$$

但注意到 $-x\log_2 x$ 在 $x=0$ 时为 0，在 $x=1$ 时也为 0。由于 $p_k$ 是概率，最小值出现在某个 $p_j=1$，其余 $p_k=0$。此时：

$$
Ent(D) = -[1 \cdot \log_2 1 + 0 + \cdots + 0] = 0
$$

**结论：**
- 当且仅当所有样本属于同一类（即某个 $p_j=1$，其余 $p_k=0$）时，信息熵 $Ent(D)$ 取得最小值 0。
- 这时集合 $D$ 的纯度最高，没有不确定性。

---

这样给出了信息熵最小值的严格数学推导。
