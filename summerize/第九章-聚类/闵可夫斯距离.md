# 闵可夫斯距离（Minkowski Distance）

闵可夫斯距离是欧氏距离和曼哈顿距离的推广形式，是一种常用的度量空间中两点之间距离的方法。

## 计算公式

对于两个 $n$ 维向量 $x = (x_1, x_2, ..., x_n)$ 和 $y = (y_1, y_2, ..., y_n)$，闵可夫斯距离的定义为：

$$
D_p(x, y) = \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}
$$

其中 $p \geq 1$，$p$ 是一个参数。

- 当 $p=1$ 时，得到曼哈顿距离：$\sum_{i=1}^n |x_i - y_i|$
- 当 $p=2$ 时，得到欧氏距离：$\sqrt{\sum_{i=1}^n (x_i - y_i)^2}$
- 当 $p \to \infty$ 时，得到切比雪夫距离：$\max_i |x_i - y_i|$

## 来源与原理

闵可夫斯距离由俄罗斯数学家赫尔曼·闵可夫斯基（Hermann Minkowski）提出，是 $L^p$ 空间范数的具体表现。它是一种广义的距离度量方式，满足距离的四个公理（非负性、对称性、三角不等式、同一性），因此可以作为度量空间的距离函数。

为什么可以这么计算？
- 闵可夫斯距离本质上是 $L^p$ 范数的推广，$p$ 不同，度量空间的“形状”不同。
- $p=1$ 时，距离是“城市街区”式的（只能沿坐标轴走），$p=2$ 时是直线距离。
- $p$ 越大，对单个坐标差异越敏感。
- 该距离满足三角不等式，因此可以作为距离度量。

## 使用场景
- **聚类分析**：如K均值、K-中心等聚类算法中，常用闵可夫斯距离度量样本间距离。
- **K近邻（KNN）分类**：KNN算法中可以根据实际需求选择不同的 $p$，如 $p=1$（曼哈顿）、$p=2$（欧氏）。
- **异常检测**：计算样本与中心的距离，判断是否为异常点。
- **多维特征空间的相似性度量**：如图像、文本、用户行为等多维数据的距离计算。

## 小结
闵可夫斯距离是一种灵活、通用的距离度量方式，通过调整 $p$，可以适应不同的数据分布和实际需求，是机器学习、数据挖掘等领域常用的基础工具。
