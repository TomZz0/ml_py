# 跳出局部极小的常见算法

神经网络训练过程中，损失函数往往存在多个局部极小点（local minima），而我们希望找到的是全局最小点（global minimum）。为此，常用以下几种算法来跳出局部极小：

---

## 1. 不同的初始参数（Random Initialization）

**原理**：多次用不同的随机初始参数训练模型，增加找到全局最小的概率。

**示例代码**：
```python
best_loss = float('inf')
for i in range(N):
    model = NeuralNetwork()
    model.random_initialize()
    model.train()
    if model.loss < best_loss:
        best_model = model
        best_loss = model.loss
```
**应用**：深度学习训练中常用，尤其是小型网络。

---

## 2. 模拟退火（Simulated Annealing）

**原理**：借鉴物理退火过程，允许以一定概率接受更差的解，从而跳出局部极小。随着“温度”降低，接受更差解的概率逐渐减小。

**示例代码**：
```python
T = T_init
while T > T_min:
    new_params = perturb(params)
    delta = loss(new_params) - loss(params)
    if delta < 0 or random() < exp(-delta / T):
        params = new_params
    T *= cooling_rate
```
**应用**：组合优化、神经网络权重优化等。

---

## 3. 随机扰动（Random Perturbation）

**原理**：在训练过程中，周期性地对参数加入小的随机扰动，帮助模型跳出局部极小。

**示例代码**：
```python
for epoch in range(num_epochs):
    train_step()
    if epoch % k == 0:
        params += np.random.normal(0, sigma, size=params.shape)
```
**应用**：深度学习、遗传算法等。

---

## 4. 演化算法（Evolutionary Algorithms）

**原理**：模拟生物进化过程，通过种群、变异、交叉等操作搜索最优解，天然具备跳出局部极小的能力。

**示例代码**：
```python
population = initialize_population()
for generation in range(max_generations):
    fitness = evaluate(population)
    selected = select(population, fitness)
    offspring = crossover_and_mutate(selected)
    population = offspring
```
**应用**：神经网络结构搜索、超参数优化等。

---

## 5. 其他方法
- 增加噪声
- 梯度裁剪
- 自适应学习率优化器（如Adam、RMSprop等）

---

## 应用场景
- 神经网络训练（避免陷入局部极小）
- 组合优化（如旅行商问题）
- 超参数搜索
- 强化学习策略优化

---

这些算法通过引入随机性、全局搜索或模拟自然过程，有效提升了找到全局最优解的概率，是深度学习和复杂优化问题中常用的策略。